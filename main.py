"""
GraphRAGé¡¹ç›®ä¸»ç¨‹åº
æ¼”ç¤ºå®Œæ•´çš„GraphRAGæµç¨‹
"""

import asyncio
import os
import argparse
from typing import List, Dict, Any

from src.utils import load_config, save_results, visualize_graph, create_evaluation_report, setup_logging
from src.rag_engine import GraphRAGEngine

async def main():
    """ä¸»å‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='GraphRAG Demo')
    parser.add_argument('--config', default='config/config.yaml', help='Config file path')
    parser.add_argument('--dataset', default='squad', choices=['squad', 'hotpot_qa', 'natural_questions'], 
                       help='Dataset to use')
    parser.add_argument('--custom-data', help='Path to custom dataset')
    parser.add_argument('--query', help='Single query to process')
    parser.add_argument('--batch-queries', help='File containing queries (one per line)')
    parser.add_argument('--output-dir', default='./output', help='Output directory')
    parser.add_argument('--visualize', action='store_true', help='Generate graph visualization')
    parser.add_argument('--log-level', default='INFO', help='Logging level')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    os.makedirs(args.output_dir, exist_ok=True)
    setup_logging(args.log_level, os.path.join(args.output_dir, 'graphrag.log'))
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    if not config:
        print("Failed to load config. Using default settings.")
        config = get_default_config()
    
    # åˆå§‹åŒ–GraphRAGå¼•æ“
    print("ğŸš€ Initializing GraphRAG Engine...")
    engine = GraphRAGEngine(config)
    
    try:
        # ä»æ•°æ®é›†åˆå§‹åŒ–
        await engine.initialize_from_dataset(args.dataset, args.custom_data)
        
        print(f"âœ… GraphRAG initialized successfully!")
        print(f"ğŸ“Š Graph Statistics:")
        stats = engine.get_graph_summary()
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        # å¯è§†åŒ–å›¾ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if args.visualize:
            print("ğŸ“ˆ Generating graph visualization...")
            viz_path = os.path.join(args.output_dir, 'graph_visualization.png')
            visualize_graph(engine.graph, viz_path)
        
        # å¤„ç†æŸ¥è¯¢
        results = []
        
        if args.query:
            # å•ä¸ªæŸ¥è¯¢
            print(f"\nğŸ” Processing query: {args.query}")
            result = await engine.query(args.query)
            results.append(result)
            
            print(f"ğŸ’¡ Answer: {result['answer']}")
            print(f"ğŸ¯ Confidence: {result['confidence']:.2f}")
            print(f"ğŸ·ï¸  Entities: {[e['text'] for e in result['entities']]}")
            
        elif args.batch_queries:
            # æ‰¹é‡æŸ¥è¯¢
            print(f"\nğŸ“ Processing batch queries from {args.batch_queries}")
            
            with open(args.batch_queries, 'r', encoding='utf-8') as f:
                queries = [line.strip() for line in f if line.strip()]
            
            results = await engine.batch_query(queries)
            
            print(f"âœ… Processed {len(results)} queries")
            
        else:
            # äº¤äº’å¼æŸ¥è¯¢
            print("\nğŸ’¬ Interactive Query Mode (type 'quit' to exit)")
            
            while True:
                query = input("\nğŸ” Enter your question: ").strip()
                
                if query.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not query:
                    continue
                
                try:
                    result = await engine.query(query)
                    
                    print(f"\nğŸ’¡ Answer: {result['answer']}")
                    print(f"ğŸ¯ Confidence: {result['confidence']:.2f}")
                    
                    if result['entities']:
                        print(f"ğŸ·ï¸  Related Entities: {[e['text'] for e in result['entities'][:5]]}")
                    
                    if result['sources']:
                        print(f"ğŸ“š Sources: {len(result['sources'])} document chunks")
                    
                    results.append(result)
                    
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ Goodbye!")
                    break
                except Exception as e:
                    print(f"âŒ Error: {e}")
        
        # ä¿å­˜ç»“æœ
        if results:
            results_path = os.path.join(args.output_dir, 'query_results.json')
            save_results(results, results_path)
            
            # åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
            report_path = os.path.join(args.output_dir, 'evaluation_report.json')
            create_evaluation_report(results, report_path)
            
            print(f"\nğŸ“„ Results saved to {results_path}")
            print(f"ğŸ“Š Evaluation report saved to {report_path}")
        
        # ä¿å­˜å›¾ç»“æ„
        graph_path = os.path.join(args.output_dir, 'knowledge_graph.json')
        engine.save_graph(graph_path)
        print(f"ğŸ•¸ï¸  Knowledge graph saved to {graph_path}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

def get_default_config() -> Dict[str, Any]:
    """è·å–é»˜è®¤é…ç½®"""
    return {
        'model': {
            'llm_model': 'gpt-3.5-turbo',
            'embedding_model': 'text-embedding-ada-002',
            'temperature': 0.1,
            'max_tokens': 2000,
            'local_llm': False,
            'local_embedding_model': 'sentence-transformers/all-MiniLM-L6-v2'
        },
        'graph': {
            'chunk_size': 1000,
            'chunk_overlap': 200,
            'max_entities_per_chunk': 10,
            'similarity_threshold': 0.8,
            'community_algorithm': 'leiden',
            'resolution': 1.0
        },
        'retrieval': {
            'top_k_entities': 20,
            'top_k_communities': 5,
            'max_context_length': 8000
        },
        'storage': {
            'working_dir': './graphrag_cache',
            'enable_cache': True
        },
        'dataset': {
            'name': 'squad',
            'max_samples': 1000,
            'split': 'train'
        }
    }

async def demo_queries() -> List[str]:
    """æ¼”ç¤ºæŸ¥è¯¢"""
    return [
        "What is machine learning?",
        "How does neural network work?",
        "What are the applications of artificial intelligence?",
        "Explain the concept of deep learning",
        "What is the difference between supervised and unsupervised learning?"
    ]

if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    # os.environ["OPENAI_API_KEY"] = "your-api-key-here"
    
    asyncio.run(main())
